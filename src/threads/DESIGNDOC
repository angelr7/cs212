            +--------------------+
            |        CS 212      |
            | PROJECT 1: THREADS |
            |   DESIGN DOCUMENT  |
            +--------------------+

---- GROUP ----

>> Fill in the names and email addresses of your group members.

Pilli Cruz-De Jesus <pilli@stanford.edu>
Angel Ruiz <ar7@stanford.edu>
Fabian Luna <luna1206@stanford.edu>

---- PRELIMINARIES ----

>> If you have any preliminary comments on your submission, notes for the
>> TAs, or extra credit, please give them here.

>> Please cite any offline or online sources you consulted while
>> preparing your submission, other than the Pintos documentation, course
>> text, lecture notes, and course staff.

                 ALARM CLOCK
                 ===========

---- DATA STRUCTURES ----

>> A1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.
>>
static struct list tick_list;
    The tick list holds all of the sleep durations for the blocked 
    threads in ascending order so our tick interrupt can wake 
    them up appropriately.

struct tick_list_item {
    int64_t ticks;
    struct thread *thread_ref;
    struct list_elem elem;
};
    This struct represents an item in the tick list. It has a
    list elem which is used to manage its position, a reference to a 
    sleeping thread, and a tick value for when the thread should wake up.

---- ALGORITHMS ----

>> A2: Briefly describe what happens in a call to timer_sleep(),
>> including the effects of the timer interrupt handler.
When a function calls timer_sleep() we first disable interrupts. Then, we
check for a valid interval (i.e > 0) and create a new tick_list_item 
using the current running thread and the tick value. We then insert our 
tick_list_item into tick_list, ordering by ascending tick duration. We then 
block the thread. 

The timer_interrupt handler at every system tick loops through tick_list
and checks if the number of OS ticks is greater than each item's sleep duration. 
We then unblock the thread and remove its entry in the tick list if needed.

>> A3: What steps are taken to minimize the amount of time spent in
>> the timer interrupt handler?
When we find the first duration that has not elapsed we stop searching. 
We also carefully monitored our allocation of stack variables so that we
do not unnecessarily use memory and increase runtime. 


---- SYNCHRONIZATION ----

>> A4: How are race conditions avoided when multiple threads call
>> timer_sleep() simultaneously?
Race conditions are avoided because upon a timer interrupt, all threads
whose duration has passed will be woken up. Since the tick_list is ordered,
all of the threads will be woken up in the correct order as well. Unblocking 
all of the threads in order also allows the simple scheduler to maintain the 
correct thread ordering. 

When we disable interrupts we prevent other threads from inconsistently modifying 
shared variables such as tick_list. We also avoid time discrepencies from calculations
using the global ticks variable.

>> A5: How are race conditions avoided when a timer interrupt occurs
>> during a call to timer_sleep()?
When this situation occurs the interrupt would not be handled because 
for most of timer_sleep() interupts are disabled. 

If an interrupt were to occur before we disabled them, our if condition 
checks to make sure that the interval has not passed when the original 
thread is pushed back on the CPU.


---- RATIONALE ----

>> A6: Why did you choose this design?  In what ways is it superior to
>> another design you considered?
When we initially approached the problem, we considered using only condition variables to 
manage sleeping threads. This quickly became a problem, mainly because it is forbidden for 
interrupt handlers to acquire locks. For this reason, our team decided to take a simpler 
approach by creating a global tick_list and disabling interrupts. This was a much better 
approach because it requires less code, and also allowed us to unblock sleeping threads in the 
timer interrupt handler. By unblocking sleeping threads inside of this interrupt, we make sure 
that we consistently wake up threads when necessary, since we check for elapsed durations upon
every OS tick.


             PRIORITY SCHEDULING
             ===================

---- DATA STRUCTURES ----

>> B1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

struct thread
{
  unsigned fifo_ordering;
  struct lock *lock_acquiring;
  struct lock *lock_releasing;
  struct list locks_holding;
};
    The added lock_acquiring and lock_releasing lock pointers keeps track of when
    a thread is in the middle of acquiring and releasing a lock. The locks_holding
    list keeps track of all the locks a thread is holding for the purposes of
    calculating priority donation values. fifo_ordering allows us to know the order
    in which threads are added to the ready queue.

struct lock
{
  struct list_elem elem;
};
    This added list_elem is used to add locks to threads' locks_holding lists.

static unsigned list_order;
    Global variable that we increment with each insertion to the ready queue
    to keep track of fifo ordering.

>> B2: Explain the data structure used to track priority donation.
>> Use ASCII art to diagram a nested donation.  (Alternately, submit a
>> .png file.)
In order to track priority donations, each thread keeps a list of all the locks
it is currently holding. When we want to get the priority of the thread, we take
the maximum between the threads priority and the maximum of the donated priorities
coming from the locks it is holding. Obtain the maximum of the donated priorities
by looping through all the locks the thread is holding and finding the maximum
priority of each locks waiters. This algorithm uses a top down approach and recursively 
calls get_thread_priority() in order to calculate the priorities of the donor threads.
This recursive call will go a level down use the same logic to retrieve the donated
priority from the donor threads own donors.

Nested Priority Graph:
https://web.stanford.edu/~luna1206/priority_donate_graph.png

Nested Priority Flow:
https://web.stanford.edu/~luna1206/priority_donate_flow.jpg

---- ALGORITHMS ----

>> B3: How do you ensure that the highest priority thread waiting for
>> a lock, semaphore, or condition variable wakes up first?
When choosing a thread to wake up, we use the list_max function to choose
the highest priority thread from the lists. Our less than function used
by list_max calls get_thread_priority to retrieve a threads priority and
returns the thread with the maximum priority found in the list. If there are
ties, they are broken using the fifo_ordering thread member variable that keeps
track of the order that a thread was added to the ready list. Thread's with a
lower fifo_ordering are considered greater because they were added to the list
earlier. The fifo_ordering is reassigned every time a thread is addded back to
the ready list using the global list_order variable that we increment with every
insertion.

>> B4: Describe the sequence of events when a call to lock_acquire()
>> causes a priority donation.  How is nested donation handled?
Within the lock_acquire function before calling sema_down, we set the
thread's lock_aquiring member to point to the lock. Within the sema_down
function, the thread will add itself to the lock semaphore's waiters list
and block. When the thread is able to continue within sema_down (acquire the lock),
it will check if it is currently aquiring a lock by checking if the thread's 
lock_acquiring member is not NULL. If it is aquiring a lock, then we add 
the lock we are acquiring to the thread's locks_holding list and set 
lock_acquiring to NULL to signify that we are done with the process of 
acquiring this lock and exit sema_down.

We handle nested donations by adding acquired locks to each thread's locks_holding
list. When retrieving the thread's priority, we utilize the locks_holding list 
to calculated the thread's donated priority by looping through the list and 
getting the donated priority coming from the waiters of those locks. In the nested 
case, we also go through the waiting threads' locks_holding lists to retrieve 
their nested donated priorities using a top down, recursive approach. No other
action is taken to handle nested donations other than adding the acquired lock to
the threads locks_holding list.

>> B5: Describe the sequence of events when lock_release() is called
>> on a lock that a higher-priority thread is waiting for.
In the lock_release function we set the thread's lock_releasing member to point to
the lock that the thread is currently holding, prior to calling sema_up. Within the
call to sema_up, we initialize a thread_unblocked bool to false. If there are threads
waiting on this semaphore, we pull the thread with the highest priority from the
waiters list using the list_max function. We remove that thread from the list and 
unblock that thread to add it back to the ready list. We set the thread_unblocked
flag to true and remove the lock from the current thread's locks_holding list using the
lock_releasing member. If we have unblockd a thread, we call thread_yield and allow the
scheduler to schedule the unblocked higher-priority thread that was just unblocked.

---- SYNCHRONIZATION ----

>> B6: Describe a potential race in thread_set_priority() and explain
>> how your implementation avoids it.  Can you use a lock to avoid
>> this race?
In thread_set_priority() there is the potential race condition where the thread
is preempted and yields while modifying its priority member variable, and the scheduler
is not able to correctly schedule the thread whose priority we were changing with
the updated value. We cannot solve this with a lock because thread_get_priority() is called on
this thread in the schedule() and next_thread_to_run() function, which run with interrupts
disabled. We cannot acquire locks while interrupts are disabled, so we must also disable
interrupts in the thread_set_priority() function.



---- RATIONALE ----

>> B7: Why did you choose this design?  In what ways is it superior to
>> another design you considered?

Previously, we had attempted to have each thread keep track of its own priority donations
list to make calls to thread_get_priority only have to compare a thread's priority member
variable to the maximum of its priority donations list. We also attempted to keep all lists
(ready_list, condition variable and semaphore waiters lists) ordered by inserting elements in 
an ordered fashion, that way we could pop from one end of the list when we wanted to get the 
maximum. This proved to be a much more complex and demanding implementation, because any change
to a threads priority or a priority donation would call for a resorting of all lists that were
affected. All changes also needed to be propagated through nested donation changes using a bottom
up approach. Furthermore, we ran into multiple bugs with priority donation lists becoming corrupted
seemingly out of nowhere. This also made it difficult to ensure round robin ordering with the
constant need to resort many lists, including the ready list.

Our implementation simplified this logic and only keeps track of the bare minimum necessary to
calculate priority donations. We implemented the locks_holding list that contains existing locks,
no longer needing to create additional priority donation elements for the priority donations list
therefore using less memory. Furthermore, rather than constantly resorting lists with any changes
made to priorities or priority donations, using a top down approach and retrieving thread priorities
utlizing threads_lock_holding lists proved to be a straightforward implementation that required
less memory. This also minimized the amount of time spent within disabled interrupts contexts in the
synchronization primitives. Previously, donation changes needed to be handled immediately within the code
for these synchronization primitives. Our final implemenation only called for a single elements list removal
or insertion when acquiring or releasing locks.

              ADVANCED SCHEDULER
              ==================

---- DATA STRUCTURES ----

>> C1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

static struct list mlfq[64];
    mlfq is the advanced scheduler's ready_list. It is an array of 64 
    lists from priority 0-63 where the index i is that lists priority

static int load_avg
    loag_avg is a real number and global variable which holds the 
    load_avg for the cpu

struct thread
{
    int nice;                          
    int recent_cpu;                    
    bool recent_cpu_changed;   
};
    nice and recent_cpu hold that threads respective nice and recent_cpu 
    values. recent_cpu_changed is a boolean which dictates if the value of
    recent_cpu has been changed since priorities were last calculated


---- ALGORITHMS ----

>> C2: Suppose threads A, B, and C have nice values 0, 1, and 2.  Each
>> has a recent_cpu value of 0.  Fill in the table below showing the
>> scheduling decision and the priority and recent_cpu values for each
>> thread after each given number of timer ticks:

This is assuming that the PRI_MAX is equal to 63.

timer  recent_cpu    priority   thread
ticks   A   B   C   A   B   C   to run
-----  --  --  --  --  --  --   ------
 0     0   0   0   63  61  59     A
 4     4   0   0   62  61  59     A
 8     8   0   0   61  61  59     B
12     8   4   0   61  60  59     A
16     12  4   0   60  60  59     B
20     12  8   0   60  59  59     A
24     16  8   0   59  59  59     C 
28     16  8   4   59  59  58     B
32     16  12  4   59  58  58     A
36     20  12  4   58  58  58     C

>> C3: Did any ambiguities in the scheduler specification make values
>> in the table uncertain?  If so, what rule did you use to resolve
>> them?  Does this match the behavior of your scheduler?

The problem above does not specify the behavior of the scheduler when two 
or more threads have the same priority. In this case, I chose to run threads
with the same priority on a round-robin basis. This is exactly what we chose
to do in our real approach as well. 

When it comes to calculating recent_cpu, priority, and load_avg, the order is
not explicity specified in the project documentation. On the threads fourth tick
we first increment recent_cpu, and then calculate the priority afterwards if necessary.
Although it isn't relevant for the problem above, we also must follow
this process on the OS's 100th tick in this order:
    -- Increment recent_cpu for running thread
    -- Calculate load_avg. This is done once, since load_avg isn't thread specific.
    -- Recalculate recent_cpu for all threads
    -- Recalculate priority for all threads where recent_cpu_changed == true

>> C4: How is the way you divided the cost of scheduling between code
>> inside and outside interrupt context likely to affect performance?

The recalculations performed every 4 and 100 ticks are done within the
interrupt context so it is important that they are done quickly to give 
as much time for the process to run as possible. In the case that our 
calculations in an interrupt context take too long we risk starving the 
process by continuously interrupting. For that reason, we decomposed
fixed-point to streamline all of the arithmetic for the compiler. We also minimize 
what we declared and initialized in the interrupt context. This minimization is
evident from our use and creation of functions such as thread_forEach, 
recalculate_recent_cpu, recalculate_load_avg, recalculate_priorities, and
recalculate_all_recent_cpus. 

Our original implementation did not have streamlined arithmetic and did not 
decompose and minimize declarations in the interrupt context. As a result, 
we experienced lots of bugs and page faults which were fixed through our final 
implementation. 


---- RATIONALE ----

>> C5: Briefly critique your design, pointing out advantages and
>> disadvantages in your design choices.  If you were to have extra
>> time to work on this part of the project, how might you choose to
>> refine or improve your design?

The advantages of our design lies in the memory optimizations that we made
in our program. For example, we use thread_foreach to streamline the operations
that we make when updating all threads. In addition, we supply a recent_cpu_changed
boolean to the thread struct, which allows us to determine if we need to recalculate 
its priority in a more lightweight manner (i.e. compared to a list).
The cons of our design mostly pertains to the way that we handle recalculations related
to thread priority. Although we tried to optimize using thread_foreach, we would be able
to further optimize our search and updating process if we used a list of threads whose 
recent_cpu was recalucated. If we were to have extra time, we would most likely spend it 
trying to optmize this process, since we unnecessarily loop over all threads by using
thread_foreach.


>> C6: The assignment explains arithmetic for fixed-point math in
>> detail, but it leaves it open to you to implement it.  Why did you
>> decide to implement it the way you did?  If you created an
>> abstraction layer for fixed-point math, that is, an abstract data
>> type and/or a set of functions or macros to manipulate fixed-point
>> numbers, why did you do so?  If not, why not?

Our initial approach was without a seperate fixed-point file. In this version we 
calculated all fixed point math in the calulate_load_avg, calculate_priority, and 
calculate_recent_cpu functions. We were running into bugs and found that creating a 
fixed point file made our thread code more readable, made it faster, and left less 
room for error in our calculations between fixed points. Also having functions for 
all fixed point arithmetic seperate was better because these fixed point operations 
were used in other locations as well. When increamenting recent_cpu every tick we 
utilized add_fp_to_int. In thread_get_load_avg and thread_get_recent_cpu we utilized 
fp_to_int_round_nearest. The multiple uses of these functions made seperating them
into their own file the best solution.  


               SURVEY QUESTIONS
               ================

Answering these questions is optional, but it will help us improve the
course in future quarters.  Feel free to tell us anything you
want--these questions are just to spur your thoughts.  You may also
choose to respond anonymously in the course evaluations at the end of
the quarter.

>> In your opinion, was this assignment, or any one of the three problems
>> in it, too easy or too hard?  Did it take too long or too little time?

>> Did you find that working on a particular part of the assignment gave
>> you greater insight into some aspect of OS design?

>> Is there some particular fact or hint we should give students in
>> future quarters to help them solve the problems?  Conversely, did you
>> find any of our guidance to be misleading?

>> Do you have any suggestions for the TAs to more effectively assist
>> students, either for future quarters or the remaining projects?

>> Any other comments?

             +-------------------------+
             |         CS 212          |
             | PROJECT 4: FILE SYSTEMS |
             |     DESIGN DOCUMENT     |
             +-------------------------+

---- GROUP ----

>> Fill in the names and email addresses of your group members.

Angel Ruiz <ar7@stanford.edu>
Pilli Cruz-De Jesus <pilli@stanford.edu>
Fabian Luna <luna1206@stanford.edu>

---- PRELIMINARIES ----

>> If you have any preliminary comments on your submission, notes for the
>> TAs, or extra credit, please give them here.

>> Please cite any offline or online sources you consulted while
>> preparing your submission, other than the Pintos documentation, course
>> text, lecture notes, and course staff.

             INDEXED AND EXTENSIBLE FILES
             ============================

---- DATA STRUCTURES ----

>> A1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

>> A2: What is the maximum size of a file supported by your inode
>> structure?  Show your work.

---- SYNCHRONIZATION ----

>> A3: Explain how your code avoids a race if two processes attempt to
>> extend a file at the same time.

When a process wants to extend a file it will be calling file_write which
calls inode_write_at. Every file has an EOF (end of file) lock and a length
lock which are accessed through the inode. The EOF lock is used to ensure 
that more than one concurrent write which extends past the end of file can not
occur. The EOF lock locks the entire write call which extends past eof, if
we are not extending past the end of file two concurrent write can occur.
We also have a length lock, which ensures that the length is not in the middle
of being changed when you attempt to grab the length. The inode_write_at
does not update the length of the inode until after all allocation and
writing has completed to ensure that no reads or writes occuring at the same
time have access to unallocated memory with garbage values.



>> A4: Suppose processes A and B both have file F open, both
>> positioned at end-of-file.  If A reads and B writes F at the same
>> time, A may read all, part, or none of what B writes.  However, A
>> may not read data other than what B writes, e.g. if B writes
>> nonzero data, A is not allowed to see all zeros.  Explain how your
>> code avoids this race.

In the situation where process A and B are both at the end of file, where
A is reading and B is writing we will have three cases which avoid race
conditions. Our implementation only updates length when a write completed
ensuring that a race or the possibility of reading garbage data is not 
possible. Also, a process will be unable to read past eof therefore an
attempt to read garbage data will not work. Since inode length is 
protected with a fine grained lock we ensure that the length changes
atomically. 


>> A5: Explain how your synchronization design provides "fairness".
>> File access is "fair" if readers cannot indefinitely block writers
>> or vice versa.  That is, many processes reading from a file cannot
>> prevent forever another process from writing the file, and many
>> processes writing to a file cannot prevent another process forever
>> from reading the file.

We allow for files to be simultaneously read and written to. The only
exception to this rule is when a file is being written past eof only
one process will be allowed to do this. This allows for maximal fairness
since we are ensuring that no read is blocking any write and that
no write is blocking a read.

---- RATIONALE ----

>> A6: Is your inode structure a multilevel index?  If so, why did you
>> choose this particular combination of direct, indirect, and doubly
>> indirect blocks?  If not, why did you choose an alternative inode
>> structure, and what advantages and disadvantages does your
>> structure have, compared to a multilevel index?

We chose a multilvel index with 12 direct blocks, 1 indirect block,
and 1 doubly indirect block. The reason for this choice was to ensure
that we satisfied the spec which stated that file sized of 8mb should
be supported through our indexing system. This setups stores more than
enough data to handle 8mb files.

                SUBDIRECTORIES
                ==============

---- DATA STRUCTURES ----

>> B1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

---- ALGORITHMS ----

>> B2: Describe your code for traversing a user-specified path.  How
>> do traversals of absolute and relative paths differ?

We have a function called parse_path which we utilize numerous times to
traverse paths. 


---- SYNCHRONIZATION ----

>> B4: How do you prevent races on directory entries?  For example,
>> only one of two simultaneous attempts to remove a single file
>> should succeed, as should only one of two simultaneous attempts to
>> create a file with the same name, and so on.

To prevent races on directory entries we utilized a directory lock. This
lock ensures that changing, removing, or creating a directory does
not happen concurrently. This will ensure that no races conditions
will occur and that there are no accidental duplicates in our system.

>> B5: Does your implementation allow a directory to be removed if it
>> is open by a process or if it is in use as a process's current
>> working directory?  If so, what happens to that process's future
>> file system operations?  If not, how do you prevent it?

Our implementation only allows for a directory to be removed if no
process has it open, it is not the working directory for any 
process, and it has no entries. In our implementation any time a process
uses a directory as its working_directory it will increase open_cnt. Also,
opening the directory regularly will increase open_cnt. Therefore 
both of these checks are handled through the value of open_cnt which
can be accessed from the directories inode. Since we open the process
twice just to look at it in the event that open_cnt <= 2 we know that
we are able to remove that directory. Otherwise we will decrement open_cnt
and will not remove the directory. 

---- RATIONALE ----

>> B6: Explain why you chose to represent the current directory of a
>> process the way you did.

FOR FABIAN AND ANGEL

                 BUFFER CACHE
                 ============

---- DATA STRUCTURES ----

>> C1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

This is the cache entry struct which is used to store the information
for each entry in our buffer cache.
struct cache_entry
{
  block_sector_t sector_idx;
  bool accessed;
  bool dirty;
  int num_active;
  unsigned char data[BLOCK_SECTOR_SIZE];
  struct lock lock;
};
The read_ahead_struct contains the sector_id so that the read ahead
thread is able to read_ahead the following sector
struct read_ahead_struct 
{
  block_sector_t sector_id; 
  struct list_elem elem;
};

List of read_ahead_structs for read ahead thread
struct list read_ahead_ids;

Condition variable to wake up read_ahead_thread
struct condition read_ahead;

lock for read_ahead_thread
struct lock read_ahead_lock;

Array of 64 cache_entries that make up our buffer cache
static struct cache_entry **buffer_cache;

Bitmap of used blocks in our cache
static struct bitmap *used_map;

Lock to be used when searching for a particular entry
static struct lock searching_lock;

Evict idx used for eviction algorithm location
static int evict_idx;


---- ALGORITHMS ----

>> C2: Describe how your cache replacement algorithm chooses a cache
>> block to evict.

We use clock eviction. Check the cache entries accessed bit if it is accessed
we set it to false and continue. Otherwise we evict that page, if dirty then 
we write it to disk. The next time we evict we continue with the entry after 
the one which we previously evicted. If an entry is pinned we skip over it and
go to the following cache entry. An entry is pinned if num_active > 0. 

>> C3: Describe your implementation of write-behind.

For write behind we always write dirty blocks to to the cache. Upon eviction
we write that block to disk. To make sure lots of data is not lost upon a 
crash we flush (write) all dirty entries in our cache to disk every 100 ticks. To 
do this we have a flush thread in the background which wakes up every 100 
ticks and flushes the cache then goes back to sleep. This thread is 
initialized in buffer_cache_init. To ensure that we are not busy waiting
we utilize our timer_sleep() implementation from assignment 1. 

>> C4: Describe your implementation of read-ahead.

To implement read ahead we have a "read ahead thread" which is initialized
in buffer_cache_init. This thread infinitely runs in the background and
utilizes a condition variable and read ahead lock. In inode_read_at 
whenever a block is being read, if the following block in this inode exists
and is going to be read in our readahead function will fetch the 
following block. The way this happens is the the inode_read_at function
acquires the read_ahead_lock, pushes the sector_id of the following 
block to the read_ahead_ids list, signals to the read ahead function
and releases the lock.

In the read_ahead_thread function we are waiting for a signal from the 
condition variable sent by inode_read_at. When we recieve the signal we
wake up and will read in the next element in our read_ahead_ids list to
the cache. Since we do not need the data in this function we are just
ensuring that this data is in the cache so the following read in
inode_read_at is faster. 

---- SYNCHRONIZATION ----

>> C5: When one process is actively reading or writing data in a
>> buffer cache block, how are other processes prevented from evicting
>> that block?

When a process is reading or writing we ensure that it can not be evicted
through pinning. Since multiple processes can read and write to the same
data in the cache we can not use a boolean as we did in the previous
assignment. Instead we utilized an int num_active, in the event that
a process is reading or writing num active is incremented and when that
call is completed it is decremented. If num_active > 0 we are unable to
evict that entry and skip it.  

>> C6: During the eviction of a block from the cache, how are other
>> processes prevented from attempting to access the block?

We have fine grained locking which is utilized for every entry. In the
event that an entry is being evicted we lock that entry utilizing
our entry->lock. This fine grained locking method ensures that During
eviction a process is unable to be accessed. 


---- RATIONALE ----

>> C7: Describe a file workload likely to benefit from buffer caching,
>> and workloads likely to benefit from read-ahead and write-behind.

Buffer caching will benefit files that constantly reuse the same sectors
when performing their reading and writing operations. For example, imagine
that we had a file that was only a few bytes long. In order to read this
file from disk, we would need to read in its entire sector, and we would
also need to do so when writing, since we write a sector at a time. By 
pulling these sectors into memory, we only need to read once, 
and all of our subsequent reads, writes, and comparisons will be done from 
working memory. This prevents us from constantly having to read from disk
when reading files and writing less than an entire sector.

Reading or writing to files with more than one sector will benefit from 
our read-ahead funtionality. By fetching the next sector attached to a file
in parallel, we place it in our buffer cache, making it available by the 
the time that the process will be able to read that next sector if necessary.
For example, on a call like "cat", we would need to read the entire contents
of the file. Reading ahead in this case would speed up our process 
tremendously,since every other sector that we're reading is brought into the 
buffer cache.This prevents our process from having to constantly wait on 
expensive IO calls to read data from the disk.

For our write behind functionality, files that constantly overwrite will 
benefit from having dirty pages in the buffer cache. For example, if we have
a file that is two sectors long, and it holds the error log for some program
(i.e. a Pintos test), there is a good chance that we will constantly 
overwrite the file over the course of a process' lifetime. By implementing 
write-behind, we can allow these dirty sectors to be overwritten and operated
on without constantly making expensive write calls to the disk. This 
maintains the functionality and changes we'd want to see from our process 
throughout the entire system, without having to make those permanent changes 
until the process dies or the buffer cache periodically flushes. If we did
not have write-behind in this case, we would constantly need to read in the 
sector from disk to get the contents of the file and preserve what we didn't
overwrite, since we can only a minimum of one sector at a time.

               SURVEY QUESTIONS
               ================

Answering these questions is optional, but it will help us improve the
course in future quarters.  Feel free to tell us anything you
want--these questions are just to spur your thoughts.  You may also
choose to respond anonymously in the course evaluations at the end of
the quarter.

>> In your opinion, was this assignment, or any one of the three problems
>> in it, too easy or too hard?  Did it take too long or too little time?

>> Did you find that working on a particular part of the assignment gave
>> you greater insight into some aspect of OS design?

>> Is there some particular fact or hint we should give students in
>> future quarters to help them solve the problems?  Conversely, did you
>> find any of our guidance to be misleading?

>> Do you have any suggestions for the TAs to more effectively assist
>> students in future quarters?

>> Any other comments?
